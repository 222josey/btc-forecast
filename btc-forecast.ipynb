{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-YcgxYIOFXz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9xyxuPrIZu6O",
        "outputId": "3549ca79-5fad-4809-ece3-700943b0b927"
      },
      "outputs": [],
      "source": [
        "file_path = 'btc.csv'\n",
        "df = pd.read_csv(file_path, header = 1)\n",
        "\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "# dropped symbol b/c its not necessary\n",
        "df = df.drop('symbol', axis=1)\n",
        "\n",
        "# swapping 'Volume BTC' and 'Volume USD' after 2018-02-27\n",
        "# for some reason they are switched after this particular day...\n",
        "swap_date = pd.Timestamp('2018-02-27')\n",
        "df_fixed = df.copy()\n",
        "mask = df_fixed['date'] <= swap_date\n",
        "btc_temp = df_fixed.loc[mask, 'Volume BTC'].copy()\n",
        "df_fixed.loc[mask, 'Volume BTC'] = df_fixed.loc[mask, 'Volume USD']\n",
        "df_fixed.loc[mask, 'Volume USD'] = btc_temp\n",
        "\n",
        "# removing 8 rows w zeros from volume btc/usd\n",
        "df_cleaned = df_fixed[df_fixed['Volume BTC'] > 0].copy()\n",
        "\n",
        "# sma - for long-term trends\n",
        "df_cleaned['SMA_7'] = df_cleaned['close'].rolling(window=7).mean()\n",
        "df_cleaned['SMA_30'] = df_cleaned['close'].rolling(window=30).mean()\n",
        "\n",
        "# ema - for short-term trends\n",
        "df_cleaned['EMA_7'] = df_cleaned['close'].ewm(span=7, adjust=False).mean()\n",
        "df_cleaned['EMA_30'] = df_cleaned['close'].ewm(span=30, adjust=False).mean()\n",
        "\n",
        "# rsi; rsi > 70 = overbought Bitcoin, rsi < 30 = oversold Bitcoin\n",
        "def calculate_rsi(data, period=14):\n",
        "    delta = data.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "    rs = gain / loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "df_cleaned['RSI'] = calculate_rsi(df_cleaned['close'])\n",
        "\n",
        "# macd\n",
        "df_cleaned['EMA_12'] = df_cleaned['close'].ewm(span=12, adjust=False).mean()\n",
        "df_cleaned['EMA_26'] = df_cleaned['close'].ewm(span=26, adjust=False).mean()\n",
        "df_cleaned['MACD'] = df_cleaned['EMA_12'] - df_cleaned['EMA_26']\n",
        "df_cleaned['MACD_signal'] = df_cleaned['MACD'].ewm(span=9, adjust=False).mean() #EMA of the MACD line\n",
        "\n",
        "# dropping n/a rows\n",
        "df_final = df_cleaned.dropna().copy()\n",
        "print(f\"Total rows after removing NaN: {len(df_final)}\")\n",
        "\n",
        "# Create a target column for the next seven day's 'close' prices\n",
        "#Predicting next day prices proved to be too easy (for Huber regression)\n",
        "df_final['target'] = df_final['close'].shift(-7)\n",
        "\n",
        "# Use prices from PREVIOUS days, not same day\n",
        "df_final['close_lag_1'] = df_final['close'].shift(1)\n",
        "df_final['close_lag_7'] = df_final['close'].shift(7)\n",
        "df_final['high_lag_1'] = df_final['high'].shift(1)\n",
        "df_final['low_lag_1'] = df_final['low'].shift(1)\n",
        "df_final['volume_lag_1'] = df_final['Volume BTC'].shift(1)\n",
        "\n",
        "# Price momentum features\n",
        "df_final['momentum_7'] = df_final['close'].shift(1) - df_final['close'].shift(8)\n",
        "df_final['momentum_30'] = df_final['close'].shift(1) - df_final['close'].shift(31)\n",
        "\n",
        "# Volatility features\n",
        "df_final['volatility_7'] = df_final['close'].rolling(7).std().shift(1)\n",
        "df_final['volatility_30'] = df_final['close'].rolling(30).std().shift(1)\n",
        "\n",
        "df_final.dropna(inplace=True)\n",
        "\n",
        "# Break date into day, month, year for trend analysis\n",
        "df_final['day'] = df_final['date'].dt.day\n",
        "df_final['month'] = df_final['date'].dt.month\n",
        "df_final['year'] = df_final['date'].dt.year\n",
        "\n",
        "# separating features (x) from label (y: close)\n",
        "feature_cols = [\n",
        "    # Lagged prices (previous days only)\n",
        "    'close_lag_1', 'close_lag_7', 'high_lag_1', 'low_lag_1', 'volume_lag_1',\n",
        "\n",
        "    # Technical indicators (already calculated from past data)\n",
        "    'SMA_7', 'SMA_30', 'EMA_7', 'EMA_30', 'RSI', 'MACD', 'MACD_signal',\n",
        "\n",
        "    # Momentum and volatility\n",
        "    'momentum_7', 'momentum_30', 'volatility_7', 'volatility_30',\n",
        "\n",
        "    # Volume\n",
        "    'Volume BTC', 'Volume USD',\n",
        "\n",
        "    # Date components\n",
        "    'day', 'month', 'year'\n",
        "]\n",
        "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
        "outlier_labels = iso_forest.fit_predict(df_final[feature_cols])\n",
        "\n",
        "# Visualize outliers before removing\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.scatter(df_final['date'][outlier_labels == 1],\n",
        "           df_final['close'][outlier_labels == 1],\n",
        "           c='blue', label='Normal', alpha=0.5, s=10)\n",
        "plt.scatter(df_final['date'][outlier_labels == -1],\n",
        "           df_final['close'][outlier_labels == -1],\n",
        "           c='red', label='Outlier', s=30, marker='x')\n",
        "plt.title('Outlier Detection in Bitcoin Prices')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Remove outliers\n",
        "df_final = df_final[outlier_labels == 1].copy()\n",
        "print(f\"Removed {sum(outlier_labels == -1)} outliers ({sum(outlier_labels == -1)/len(outlier_labels)*100:.2f}%)\")\n",
        "print(f\"Remaining rows: {len(df_final)}\")\n",
        "\n",
        "# CRITICAL FIX: Sort by date before splitting\n",
        "df_final = df_final.sort_values('date').reset_index(drop=True)\n",
        "print(f\"\\n✅ Data sorted by date\")\n",
        "print(f\"Date range: {df_final['date'].min()} to {df_final['date'].max()}\")\n",
        "\n",
        "# Separate features and target\n",
        "y = df_final['target']\n",
        "\n",
        "# ADD PERCENTAGE CHANGE TARGET\n",
        "df_final['target_pct'] = ((df_final['target'] - df_final['close']) / df_final['close']) * 100\n",
        "\n",
        "# Split points\n",
        "train_size = int(len(df_final) * 0.7)\n",
        "val_size = int(len(df_final) * 0.2)\n",
        "\n",
        "# Splitting data (chronologically correct!)\n",
        "train = df_final[:train_size].copy()\n",
        "val = df_final[train_size:train_size + val_size].copy()\n",
        "test = df_final[train_size + val_size:].copy()\n",
        "\n",
        "# Verify splits are correct\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SPLIT VERIFICATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Train: {train['date'].min()} to {train['date'].max()}\")\n",
        "print(f\"  Mean price: ${train['close'].mean():.2f}\")\n",
        "print(f\"Val: {val['date'].min()} to {val['date'].max()}\")\n",
        "print(f\"  Mean price: ${val['close'].mean():.2f}\")\n",
        "print(f\"Test: {test['date'].min()} to {test['date'].max()}\")\n",
        "print(f\"  Mean price: ${test['close'].mean():.2f}\")\n",
        "\n",
        "\n",
        "# standardize data for Linear/Huber Regression\n",
        "# calculate mean and std from training data\n",
        "x_train = train[feature_cols]\n",
        "y_train = train['target']\n",
        "\n",
        "x_val = val[feature_cols]\n",
        "y_val = val['target']\n",
        "\n",
        "x_test = test[feature_cols]\n",
        "y_test = test['target']\n",
        "\n",
        "x_mean = x_train.mean()\n",
        "x_std = x_train.std().replace(0, 1) # ensuring no 0 values cause NaN errors\n",
        "\n",
        "# standardize train, val, test\n",
        "x_train_scaled = (x_train - x_mean) / x_std\n",
        "x_val_scaled = (x_val - x_mean) / x_std\n",
        "x_test_scaled = (x_test - x_mean) / x_std\n",
        "# print standardized dataset\n",
        "print(x_train_scaled.head())\n",
        "\n",
        "# checking splits\n",
        "print(f\"\\nTraining set: {len(train)} rows ({len(train)/len(df_final)*100:.1f}%)\")\n",
        "print(f\"Validation set: {len(val)} rows ({len(val)/len(df_final)*100:.1f}%)\")\n",
        "print(f\"Testing set: {len(test)} rows ({len(test)/len(df_final)*100:.1f}%)\")\n",
        "\n",
        "# date ranges\n",
        "print(f\"\\nTrain dates: {train.index.min()} to {train.index.max()}\")\n",
        "print(f\"Val dates: {val.index.min()} to {val.index.max()}\")\n",
        "print(f\"Test dates: {test.index.min()} to {test.index.max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdQpQm2sufi-"
      },
      "source": [
        "## EDA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "c46zY9EViHx4",
        "outputId": "7ce03534-cb97-48e7-a41a-745f7871379f"
      },
      "outputs": [],
      "source": [
        "# monthly growth of btc over time\n",
        "df_final['date'] = pd.to_datetime(df_final['date'])\n",
        "monthly_df = df_final.resample('M', on='date').agg({\n",
        "    'close': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(monthly_df['date'], monthly_df['close'], label='Monthly Average Close', color='gold')\n",
        "plt.title('Bitcoin Monthly Growth Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('BTC Price (USD)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "fNbmMM0zkKTU",
        "outputId": "675a0e32-8fe9-4985-c019-5a4d5a576c21"
      },
      "outputs": [],
      "source": [
        "# monthly % change of btc over time\n",
        "monthly_df['monthly_return_%'] = monthly_df['close'].pct_change() * 100\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.bar(monthly_df['date'], monthly_df['monthly_return_%'], color='teal', alpha=0.7)\n",
        "plt.axhline(0, color='black', linewidth=1)\n",
        "plt.title('Monthly BTC % Change')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Monthly Change (%)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "id": "zQMA5_h9sCu6",
        "outputId": "c0aa8a90-591b-406c-c457-64b368161052"
      },
      "outputs": [],
      "source": [
        "# yearly averages\n",
        "yearly_df = df_final.resample('Y', on='date')['close'].mean()\n",
        "yearly_df.plot(kind='bar', color='orange', figsize=(12,6), title='Average BTC Price by Year')\n",
        "plt.ylabel('BTC Price (USD)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "BqiOMeuqtV05",
        "outputId": "0c5868b3-2d09-4800-d712-15766d677590"
      },
      "outputs": [],
      "source": [
        "# distribution insights\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.histplot(df_final['close'], bins=50, kde=True)\n",
        "plt.title('Distribution of BTC Closing Prices')\n",
        "plt.xlabel('Price (USD)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "PsrYDU0buDU6",
        "outputId": "306d4d3d-b826-4ef9-b721-bd2b323effd9"
      },
      "outputs": [],
      "source": [
        "# rolling volatility\n",
        "df_final['volatility_30'] = df_final['close'].rolling(window=30).std()\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(df_final['date'], df_final['volatility_30'], color='red', label='30-Day Volatility')\n",
        "plt.title('BTC 30-Day Rolling Volatility')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Volatility (USD)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "cWIK6XPFuax8",
        "outputId": "51788daf-84a1-4380-9f9a-b993e6fc7218"
      },
      "outputs": [],
      "source": [
        "# sma bands\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(df_final['date'], df_final['close'], label='Close', color='gold')\n",
        "plt.plot(df_final['date'], df_final['SMA_30'], label='SMA 30', color='red')\n",
        "plt.fill_between(df_final['date'],\n",
        "                 df_final['SMA_30'] - df_final['volatility_30'],\n",
        "                 df_final['SMA_30'] + df_final['volatility_30'],\n",
        "                 color='lightblue', alpha=0.9, label='Volatility Band')\n",
        "plt.title('BTC Close with 30-Day SMA and Volatility Bands')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "2FmQBO0J6lJi",
        "outputId": "bfd77ca7-ff08-4be4-f698-637e93faecfe"
      },
      "outputs": [],
      "source": [
        "# volume and price comparison\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(df_final['date'], df_final['close'], color='gold', label='Close Price')\n",
        "plt.bar(df_final['date'], df_final['Volume BTC'], color='gray', alpha=0.5, width=3, label='Volume (BTC)')\n",
        "plt.title('BTC Price vs Volume')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price / Volume')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wft182sOLX72",
        "outputId": "83d4e227-491b-42d2-9741-cd98ad65842b"
      },
      "outputs": [],
      "source": [
        "# Implementing HuberRegressor\n",
        "huber = HuberRegressor(\n",
        "    epsilon=1.35,\n",
        "    max_iter=10000,\n",
        "    alpha=0.0001\n",
        ")\n",
        "huber.fit(x_train_scaled, y_train)\n",
        "y_pred_huber = huber.predict(x_val_scaled)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(val['date'], y_val, label=\"Actual\", linewidth=1)\n",
        "plt.plot(val['date'], y_pred_huber, label=\"Predicted\", linewidth=1, alpha=0.8)\n",
        "plt.legend()\n",
        "plt.title(\"Huber: Actual vs Predicted\")\n",
        "plt.show()\n",
        "\n",
        "# Testing absolute error\n",
        "abs_error = abs(y_val - y_pred_huber)\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(val['date'], abs_error)\n",
        "plt.title(\"Huber: Absolute Error\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pmsPxAUTzsT8",
        "outputId": "d4898c87-11c4-4276-b5ba-e95eb0dd1c6e"
      },
      "outputs": [],
      "source": [
        "# Define the evaluation function\n",
        "def evaluate_model(name, y_true, y_pred_values):\n",
        "    min_len = min(len(y_true), len(y_pred_values))\n",
        "    y_true_aligned = y_true.reset_index(drop=True)[:min_len] if hasattr(y_true, 'reset_index') else y_true[:min_len]\n",
        "    y_pred_aligned = y_pred_values[:min_len]\n",
        "\n",
        "    mae = mean_absolute_error(y_true_aligned, y_pred_aligned)\n",
        "    mse = mean_squared_error(y_true_aligned, y_pred_aligned)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true_aligned, y_pred_aligned)\n",
        "\n",
        "    print(f\"--- {name} Results ---\")\n",
        "    print(f\"MAE: {mae:.2f}\")\n",
        "    print(f\"MSE: {mse:.2f}\")\n",
        "    print(f\"RMSE: {rmse:.2f}\")\n",
        "    print(f\"R^2: {r2:.2f}\")\n",
        "    print(\"-\" * (len(name) + 11))\n",
        "\n",
        "# Evaluate Huber Regression\n",
        "evaluate_model(\"Huber Regression\", y_val, y_pred_huber)\n",
        "\n",
        "# Linear Regression\n",
        "lin = LinearRegression()\n",
        "lin.fit(x_train_scaled, y_train)\n",
        "y_pred_lin = lin.predict(x_val_scaled)\n",
        "evaluate_model(\"Linear Regression\", y_val, y_pred_lin)\n",
        "\n",
        "# Random Forest with Percentage Change\n",
        "print(\"\\nStarting Random Forest hyperparameter tuning (on percentage change)...\")\n",
        "param_grid = {\n",
        "    'n_estimators': [200, 300],\n",
        "    'max_depth': [20, 30, None],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'max_features': ['sqrt', 0.5]\n",
        "}\n",
        "\n",
        "rf_grid = GridSearchCV(\n",
        "    RandomForestRegressor(random_state=42),\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='neg_mean_absolute_error',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train on percentage change\n",
        "y_train_pct = train['target_pct']\n",
        "y_val_pct = val['target_pct']\n",
        "\n",
        "rf_grid.fit(x_train, y_train_pct)\n",
        "print(f\"\\nBest parameters: {rf_grid.best_params_}\")\n",
        "\n",
        "# Use best model\n",
        "rf = rf_grid.best_estimator_\n",
        "y_pred_rf_pct = rf.predict(x_val)\n",
        "\n",
        "# Convert percentage predictions back to prices\n",
        "val_close = val['close'].reset_index(drop=True)\n",
        "y_pred_rf = val_close * (1 + y_pred_rf_pct / 100)\n",
        "y_val_price = val_close * (1 + y_val_pct.reset_index(drop=True) / 100)\n",
        "\n",
        "# Evaluate on converted prices\n",
        "evaluate_model(\"Random Forest (Tuned - Pct Change)\", y_val_price, y_pred_rf)\n",
        "\n",
        "# Feature Importance Analysis\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'importance': rf.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\n--- Feature Importance ---\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Visualize feature importance\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Random Forest Feature Importance')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Comparison plots\n",
        "min_len_plot = min(len(y_val), len(y_pred_huber), len(y_pred_lin), len(y_pred_rf))\n",
        "val_date_aligned = val['date'].reset_index(drop=True)[:min_len_plot]\n",
        "y_val_plot_aligned = y_val.reset_index(drop=True)[:min_len_plot]\n",
        "y_pred_huber_plot_aligned = y_pred_huber[:min_len_plot]\n",
        "y_pred_lin_plot_aligned = y_pred_lin[:min_len_plot]\n",
        "y_pred_rf_plot_aligned = y_pred_rf[:min_len_plot]\n",
        "\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.plot(val_date_aligned, y_val_plot_aligned, label=\"Actual\", linewidth=2, color='blue')\n",
        "plt.plot(val_date_aligned, y_pred_lin_plot_aligned, label=\"Linear Predicted\", linewidth=1.5, alpha=0.8, color='red', linestyle='--')\n",
        "plt.title(\"Actual vs Linear Predicted Prices\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.plot(val_date_aligned, y_val_plot_aligned, label=\"Actual\", linewidth=2, color='blue')\n",
        "plt.plot(val_date_aligned, y_pred_rf_plot_aligned, label=\"Random Forest Predicted\", linewidth=1.5, alpha=0.8, color='purple', linestyle=':')\n",
        "plt.title(\"Actual vs Random Forest Predicted Prices\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.plot(val_date_aligned, y_val_plot_aligned, label=\"Actual\", linewidth=2, color='blue')\n",
        "plt.plot(val_date_aligned, y_pred_huber_plot_aligned, label=\"Huber Predicted\", linewidth=1.5, alpha=0.8, color='green')\n",
        "plt.plot(val_date_aligned, y_pred_lin_plot_aligned, label=\"Linear Predicted\", linewidth=1.5, alpha=0.8, color='red', linestyle='--')\n",
        "plt.plot(val_date_aligned, y_pred_rf_plot_aligned, label=\"Random Forest Predicted\", linewidth=1.5, alpha=0.8, color='purple', linestyle=':')\n",
        "plt.title(\"Actual vs Predicted Prices (Huber vs Linear vs Random Forest)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cUtyuZbT1ZTR",
        "outputId": "85297100-8fad-4ce3-8764-0933ea5a879f"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# LSTM Model Data Preparation\n",
        "lstm_feature_cols = ['open', 'high', 'low', 'close', 'Volume BTC', 'Volume USD', 'SMA_7', 'SMA_30', 'EMA_7', 'EMA_30', 'RSI', 'MACD', 'MACD_signal', 'day', 'month', 'year']\n",
        "\n",
        "# Scale features and target separately for LSTM\n",
        "scaler_lstm_features = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_lstm_features = scaler_lstm_features.fit_transform(df_final[lstm_feature_cols])\n",
        "\n",
        "scaler_lstm_target = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_lstm_target = scaler_lstm_target.fit_transform(df_final['target'].values.reshape(-1, 1))\n",
        "\n",
        "# Function to create LSTM sequences\n",
        "look_back_window = 30\n",
        "\n",
        "def create_lstm_sequences(features_data, target_data, look_back):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(features_data) - look_back):\n",
        "        X_seq.append(features_data[i:(i + look_back), :])\n",
        "        y_seq.append(target_data[i + look_back, 0])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# Generate sequences\n",
        "X_lstm_sequences, y_lstm_single_targets = create_lstm_sequences(scaled_lstm_features, scaled_lstm_target, look_back_window)\n",
        "y_lstm_single_targets = y_lstm_single_targets.reshape(-1, 1)\n",
        "\n",
        "# Split the sequential data\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.2\n",
        "\n",
        "train_size_lstm_split = int(len(X_lstm_sequences) * train_ratio)\n",
        "val_size_lstm_split = int(len(X_lstm_sequences) * val_ratio)\n",
        "\n",
        "x_train_lstm = X_lstm_sequences[:train_size_lstm_split]\n",
        "y_train_lstm = y_lstm_single_targets[:train_size_lstm_split]\n",
        "\n",
        "x_val_lstm = X_lstm_sequences[train_size_lstm_split : train_size_lstm_split + val_size_lstm_split]\n",
        "y_val_lstm = y_lstm_single_targets[train_size_lstm_split : train_size_lstm_split + val_size_lstm_split]\n",
        "\n",
        "x_test_lstm = X_lstm_sequences[train_size_lstm_split + val_size_lstm_split :]\n",
        "y_test_lstm = y_lstm_single_targets[train_size_lstm_split + val_size_lstm_split :]\n",
        "\n",
        "# LSTM Model Definition\n",
        "num_features_lstm = scaled_lstm_features.shape[1]\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(look_back_window, num_features_lstm)),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32, return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.summary()\n",
        "\n",
        "# Model Training\n",
        "history = model.fit(\n",
        "    x_train_lstm, y_train_lstm,\n",
        "    epochs=20,\n",
        "    batch_size=15,\n",
        "    validation_data=(x_val_lstm, y_val_lstm),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Make Predictions\n",
        "y_pred_lstm_scaled = model.predict(x_val_lstm)\n",
        "\n",
        "# Inverse transform\n",
        "pred_val_lstm = scaler_lstm_target.inverse_transform(y_pred_lstm_scaled)\n",
        "true_val_lstm = scaler_lstm_target.inverse_transform(y_val_lstm)\n",
        "\n",
        "# Evaluate LSTM\n",
        "evaluate_model(\"LSTM\", true_val_lstm.flatten(), pred_val_lstm.flatten())\n",
        "\n",
        "# Plotting LSTM predictions\n",
        "val_dates_start_idx_df = look_back_window + train_size_lstm_split\n",
        "val_dates_end_idx_df = look_back_window + train_size_lstm_split + val_size_lstm_split\n",
        "\n",
        "val_dates_for_lstm_plot = df_final['date'].iloc[val_dates_start_idx_df : val_dates_end_idx_df]\n",
        "\n",
        "min_len_plot_lstm_final = min(len(val_dates_for_lstm_plot), len(true_val_lstm.flatten()))\n",
        "val_dates_for_lstm_plot_aligned = val_dates_for_lstm_plot.head(min_len_plot_lstm_final)\n",
        "true_val_lstm_aligned = true_val_lstm.flatten()[:min_len_plot_lstm_final]\n",
        "pred_val_lstm_aligned = pred_val_lstm.flatten()[:min_len_plot_lstm_final]\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(val_dates_for_lstm_plot_aligned, true_val_lstm_aligned, label=\"Actual (LSTM Val)\", linewidth=1, color='blue')\n",
        "plt.plot(val_dates_for_lstm_plot_aligned, pred_val_lstm_aligned, label=\"Predicted (LSTM Val)\", linewidth=1, alpha=0.8, color='orange')\n",
        "plt.title(\"LSTM Actual vs Predicted Bitcoin Prices (Validation Set)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price (USD)\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PGzigUvNh-L",
        "outputId": "04c6175e-c8af-4145-bdcf-9cd58d2458b3"
      },
      "outputs": [],
      "source": [
        "# Test Huber\n",
        "y_pred_test_huber = huber.predict(x_test_scaled)\n",
        "evaluate_model(\"Huber Regression - TEST SET\", y_test, y_pred_test_huber)\n",
        "\n",
        "# Test Linear\n",
        "y_pred_test_lin = lin.predict(x_test_scaled)\n",
        "evaluate_model(\"Linear Regression - TEST SET\", y_test, y_pred_test_lin)\n",
        "\n",
        "# Test Random Forest - Convert percentage to price\n",
        "y_test_pct = test['target_pct']\n",
        "y_pred_rf_test_pct = rf.predict(x_test)\n",
        "\n",
        "test_close = test['close'].reset_index(drop=True)\n",
        "y_pred_test_rf = test_close * (1 + y_pred_rf_test_pct / 100)\n",
        "y_test_price = test_close * (1 + y_test_pct.reset_index(drop=True) / 100)\n",
        "\n",
        "evaluate_model(\"Random Forest - TEST SET (Pct Change)\", y_test_price, y_pred_test_rf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "T5eilPEaNmhx",
        "outputId": "c64906ff-4b97-4c5c-b82b-48e9946fcad7"
      },
      "outputs": [],
      "source": [
        "# Residual plot for Huber (best model)\n",
        "residuals = y_val.reset_index(drop=True) - y_pred_huber\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.scatter(y_val.reset_index(drop=True), residuals, alpha=0.5)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.xlabel('Actual Price')\n",
        "plt.ylabel('Residual (Actual - Predicted)')\n",
        "plt.title('Huber Regression: Residual Plot')\n",
        "plt.show()\n",
        "\n",
        "# Distribution of residuals\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(residuals, bins=50, kde=True)\n",
        "plt.title('Distribution of Prediction Errors')\n",
        "plt.xlabel('Residual')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x4pJc5k0SgZ9",
        "outputId": "dc9d8f93-4bbc-4d4f-fd47-1977feb71017"
      },
      "outputs": [],
      "source": [
        "# Check what dates are in each split\n",
        "print(\"=\" * 60)\n",
        "print(\"DATA SPLIT ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Train: {train['date'].min()} to {train['date'].max()}\")\n",
        "print(f\"  Price range: ${train['close'].min():.2f} - ${train['close'].max():.2f}\")\n",
        "print(f\"  Mean price: ${train['close'].mean():.2f}\")\n",
        "\n",
        "print(f\"\\nVal: {val['date'].min()} to {val['date'].max()}\")\n",
        "print(f\"  Price range: ${val['close'].min():.2f} - ${val['close'].max():.2f}\")\n",
        "print(f\"  Mean price: ${val['close'].mean():.2f}\")\n",
        "\n",
        "print(f\"\\nTest: {test['date'].min()} to {test['date'].max()}\")\n",
        "print(f\"  Price range: ${test['close'].min():.2f} - ${test['close'].max():.2f}\")\n",
        "print(f\"  Mean price: ${test['close'].mean():.2f}\")\n",
        "\n",
        "print(f\"\\nTarget ranges:\")\n",
        "print(f\"Train target: ${y_train.min():.2f} - ${y_train.max():.2f}\")\n",
        "print(f\"Val target: ${y_val.min():.2f} - ${y_val.max():.2f}\")\n",
        "print(f\"Test target: ${y_test.min():.2f} - ${y_test.max():.2f}\")\n",
        "\n",
        "# Plot train/val/test splits\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.plot(train['date'], train['close'], label='Train', alpha=0.7)\n",
        "plt.plot(val['date'], val['close'], label='Val', alpha=0.7)\n",
        "plt.plot(test['date'], test['close'], label='Test', alpha=0.7, linewidth=2)\n",
        "plt.axvline(train['date'].max(), color='red', linestyle='--', alpha=0.5)\n",
        "plt.axvline(val['date'].max(), color='red', linestyle='--', alpha=0.5)\n",
        "plt.title('Train/Val/Test Split Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('BTC Price')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Check predictions vs actual on test set\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.plot(test['date'], y_test.reset_index(drop=True), label='Actual', linewidth=2, color='blue')\n",
        "plt.plot(test['date'], y_pred_test_huber, label='Huber Predicted', alpha=0.7, color='green')\n",
        "plt.plot(test['date'], y_pred_test_lin, label='Linear Predicted', alpha=0.7, color='red')\n",
        "plt.title('Test Set: Actual vs Predicted')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "id": "X3v_Jg8D1k2p",
        "outputId": "f45c0290-f6c3-4e91-a003-e21a984b3ad8"
      },
      "outputs": [],
      "source": [
        "# Model comparison data\n",
        "models = ['Huber\\nRegression', 'Linear\\nRegression', 'Random Forest\\n(Pct Change)', 'LSTM']\n",
        "\n",
        "# Validation set metrics\n",
        "val_mae = [1639.24, 1646.39, 1631.65, 3193.65]\n",
        "val_r2 = [0.92, 0.92, 0.92, 0.81]\n",
        "\n",
        "# Test set metrics\n",
        "test_mae = [4028.37, 4168.63, 4961.29, None]\n",
        "test_r2 = [0.91, 0.90, 0.86, None]\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# MAE Comparison\n",
        "x = np.arange(len(models))\n",
        "width = 0.35\n",
        "\n",
        "axes[0].bar(x - width/2, val_mae, width, label='Validation', color='skyblue', edgecolor='black')\n",
        "axes[0].bar(x[:3] + width/2, test_mae[:3], width, label='Test', color='coral', edgecolor='black')\n",
        "axes[0].set_xlabel('Model', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('MAE (USD)', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('Mean Absolute Error Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].set_xticklabels(models)\n",
        "axes[0].legend()\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "axes[1].bar(x[:3] - width/2, [val_r2[i] for i in range(3)], width, label='Validation', color='lightgreen', edgecolor='black')\n",
        "axes[1].bar(x[:3] + width/2, test_r2[:3], width, label='Test', color='gold', edgecolor='black')\n",
        "\n",
        "axes[1].bar(x[3], val_r2[3], width, label='LSTM (Val only)', color='lightgreen', edgecolor='black', hatch='//')\n",
        "axes[1].set_xlabel('Model', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('R² Score', fontsize=12, fontweight='bold')\n",
        "axes[1].set_title('R² Score Comparison', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xticks(x)\n",
        "axes[1].set_xticklabels(models)\n",
        "axes[1].legend()\n",
        "axes[1].set_ylim([0, 1])\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "\n",
        "summary_text = \"\"\"Test Set Performance (Feb 2024 - Jun 2025):\n",
        "\n",
        "Price Range: $42,570 - $108,980  |  • Best Model: Huber Regression (R²=0.91, MAE=$4,028)  |  • All models achieve <7% prediction error\"\"\"\n",
        "\n",
        "fig.text(0.5, -0.02, summary_text,\n",
        "ha='center', fontsize=10,\n",
        "bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5, pad=10))\n",
        "plt.savefig('model_comparison_with_summary.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8687321",
        "outputId": "4433c258-c72b-4743-85c1-30e8727613f4"
      },
      "outputs": [],
      "source": [
        "df_final = df_final.sort_values(by='date', ascending=True).reset_index(drop=True)\n",
        "print(\"df_final sorted by date in ascending order.\")\n",
        "print(df_final[['date', 'close']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ni1oqiuj1u5m",
        "outputId": "c79e6cd8-9e22-4cc0-da42-43fff7c98f05"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# --- LSTM Model Definition ---\n",
        "num_features_lstm = scaled_lstm_features.shape[1] # Get the number of features from scaled data\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(look_back_window, num_features_lstm)), # Input shape: (timesteps, num_features)\n",
        "    LSTM(64, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    LSTM(32, return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)  # Predict target value (Bitcoin closing price 7 days ahead)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.summary()\n",
        "es = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4)\n",
        "\n",
        "# --- Model Training ---\n",
        "history = model.fit(\n",
        "    x_train_lstm, y_train_lstm,\n",
        "    epochs=20,\n",
        "    batch_size=14,\n",
        "    validation_data=(x_val_lstm, y_val_lstm),\n",
        "    callbacks = [es, rlr],\n",
        "    verbose = 1\n",
        ")\n",
        "\n",
        "# --- Make Predictions ---\n",
        "y_pred_lstm_scaled = model.predict(x_val_lstm)\n",
        "\n",
        "print(\"y_val mean:\", y_lstm_single_targets.mean())\n",
        "print(\"y_pred mean:\", y_pred_lstm_scaled.mean())\n",
        "\n",
        "# Inverse transform scaled predictions and true values back to original scale\n",
        "pred_val_lstm = scaler_lstm_target.inverse_transform(y_pred_lstm_scaled)\n",
        "true_val_lstm = scaler_lstm_target.inverse_transform(y_val_lstm)\n",
        "\n",
        "\n",
        "# --- Evaluate Metrics & Plot Results ---\n",
        "# The evaluate_model function (defined earlier in pmsPxAUTzsT8) expects 1D arrays.\n",
        "# Flatten the (samples, 1) output from inverse_transform.\n",
        "evaluate_model(\"LSTM (New Features)\", true_val_lstm.flatten(), pred_val_lstm.flatten())\n",
        "\n",
        "# Plotting prediction versus actual values\n",
        "# Align dates correctly for the validation set used for LSTM.\n",
        "# The first date corresponding to y_lstm_single_targets[0] is df_final['date'].iloc[look_back_window].\n",
        "# The validation set dates start from df_final['date'].iloc[look_back_window + train_size_lstm_split].\n",
        "\n",
        "val_dates_start_idx_df = look_back_window + train_size_lstm_split\n",
        "val_dates_end_idx_df = look_back_window + train_size_lstm_split + val_size_lstm_split\n",
        "\n",
        "val_dates_for_lstm_plot = df_final['date'].iloc[val_dates_start_idx_df : val_dates_end_idx_df]\n",
        "\n",
        "# Ensure lengths match for plotting by potentially truncating if needed\n",
        "min_len_plot_lstm_final = min(len(val_dates_for_lstm_plot), len(true_val_lstm.flatten()))\n",
        "val_dates_for_lstm_plot_aligned = val_dates_for_lstm_plot.head(min_len_plot_lstm_final)\n",
        "true_val_lstm_aligned = true_val_lstm.flatten()[:min_len_plot_lstm_final]\n",
        "pred_val_lstm_aligned = pred_val_lstm.flatten()[:min_len_plot_lstm_final]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(val_dates_for_lstm_plot_aligned, true_val_lstm_aligned, label=\"Actual (LSTM Val)\", linewidth=1, color='blue')\n",
        "plt.plot(val_dates_for_lstm_plot_aligned, pred_val_lstm_aligned, label=\"Predicted (LSTM Val)\", linewidth=1, alpha=0.8, color='orange')\n",
        "plt.title(\"LSTM Actual vs Predicted Bitcoin Prices (Validation Set) with New Features\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price (USD)\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2VaCaN1-1xY2",
        "outputId": "cdb56722-b307-4e19-bc3d-11daf6a7daf4"
      },
      "outputs": [],
      "source": [
        "look_back_window = 60 # Increased from 30 to 60\n",
        "\n",
        "# Re-generate sequences from the scaled data with the new look_back_window\n",
        "X_lstm_sequences, y_lstm_single_targets = create_lstm_sequences(scaled_lstm_features, scaled_lstm_target, look_back_window)\n",
        "y_lstm_single_targets = y_lstm_single_targets.reshape(-1, 1)\n",
        "\n",
        "# Re-split the sequential data into train/val/test (maintaining temporal order)\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.2\n",
        "\n",
        "train_size_lstm_split = int(len(X_lstm_sequences) * train_ratio)\n",
        "val_size_lstm_split = int(len(X_lstm_sequences) * val_ratio)\n",
        "\n",
        "x_train_lstm = X_lstm_sequences[:train_size_lstm_split]\n",
        "y_train_lstm = y_lstm_single_targets[:train_size_lstm_split]\n",
        "\n",
        "x_val_lstm = X_lstm_sequences[train_size_lstm_split : train_size_lstm_split + val_size_lstm_split]\n",
        "y_val_lstm = y_lstm_single_targets[train_size_lstm_split : train_size_lstm_split + val_size_lstm_split]\n",
        "\n",
        "x_test_lstm = X_lstm_sequences[train_size_lstm_split + val_size_lstm_split :]\n",
        "y_test_lstm = y_lstm_single_targets[train_size_lstm_split + val_size_lstm_split :]\n",
        "\n",
        "# --- LSTM Model Definition with new hyperparameters ---\n",
        "num_features_lstm = scaled_lstm_features.shape[1]\n",
        "\n",
        "model_tuned = Sequential([\n",
        "    Input(shape=(look_back_window, num_features_lstm)),\n",
        "    LSTM(128, return_sequences=True), # Increased units from 64 to 128\n",
        "    Dropout(0.3), # Increased dropout from 0.2 to 0.3\n",
        "\n",
        "    LSTM(64, return_sequences=False), # Increased units from 32 to 64\n",
        "    Dropout(0.3), # Increased dropout from 0.2 to 0.3\n",
        "\n",
        "    Dense(64, activation='relu'), # Increased units from 32 to 64\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model_tuned.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_tuned.summary()\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5) # Adjusted factor and patience\n",
        "\n",
        "# --- Model Training with new batch size ---\n",
        "history_tuned = model_tuned.fit(\n",
        "    x_train_lstm, y_train_lstm,\n",
        "    epochs=20,\n",
        "    batch_size=32, # Changed batch size from 14 to 32\n",
        "    validation_data=(x_val_lstm, y_val_lstm),\n",
        "    callbacks=[es, rlr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --- Make Predictions ---\n",
        "y_pred_lstm_scaled_tuned = model_tuned.predict(x_val_lstm)\n",
        "\n",
        "# Inverse transform scaled predictions and true values back to original scale\n",
        "pred_val_lstm_tuned = scaler_lstm_target.inverse_transform(y_pred_lstm_scaled_tuned)\n",
        "true_val_lstm_tuned = scaler_lstm_target.inverse_transform(y_val_lstm)\n",
        "\n",
        "# --- Evaluate Metrics & Plot Results ---\n",
        "evaluate_model(\"LSTM (Tuned Hyperparameters)\", true_val_lstm_tuned.flatten(), pred_val_lstm_tuned.flatten())\n",
        "\n",
        "# Align dates correctly for the validation set used for LSTM.\n",
        "val_dates_start_idx_df = look_back_window + train_size_lstm_split\n",
        "val_dates_end_idx_df = look_back_window + train_size_lstm_split + val_size_lstm_split\n",
        "\n",
        "val_dates_for_lstm_plot = df_final['date'].iloc[val_dates_start_idx_df : val_dates_end_idx_df]\n",
        "\n",
        "min_len_plot_lstm_final = min(len(val_dates_for_lstm_plot), len(true_val_lstm_tuned.flatten()))\n",
        "val_dates_for_lstm_plot_aligned = val_dates_for_lstm_plot.head(min_len_plot_lstm_final)\n",
        "true_val_lstm_aligned = true_val_lstm_tuned.flatten()[:min_len_plot_lstm_final]\n",
        "pred_val_lstm_aligned = pred_val_lstm_tuned.flatten()[:min_len_plot_lstm_final]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(val_dates_for_lstm_plot_aligned, true_val_lstm_aligned, label=\"Actual (LSTM Val)\", linewidth=1, color='blue')\n",
        "plt.plot(val_dates_for_lstm_plot_aligned, pred_val_lstm_aligned, label=\"Predicted (LSTM Val)\", linewidth=1, alpha=0.8, color='green')\n",
        "plt.title(\"LSTM Actual vs Predicted Bitcoin Prices (Validation Set) with Tuned Features\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price (USD)\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "DctH_FhQ11FM",
        "outputId": "341786bf-2814-4b3c-af0a-ac42445f879b"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL TRUTH: TUNED LSTM ON TEST SET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. Generate sequences for the TEST set using the NEW features\n",
        "# Note: We must ensure x_test_lstm matches the tuned model's input shape\n",
        "y_pred_lstm_test_scaled_tuned = model_tuned.predict(x_test_lstm)\n",
        "\n",
        "# 2. Inverse transform\n",
        "pred_test_lstm_tuned = scaler_lstm_target.inverse_transform(y_pred_lstm_test_scaled_tuned)\n",
        "true_test_lstm_tuned = scaler_lstm_target.inverse_transform(y_test_lstm)\n",
        "\n",
        "# 3. Evaluate\n",
        "evaluate_model(\"LSTM (Tuned) - TEST SET\", true_test_lstm_tuned.flatten(), pred_test_lstm_tuned.flatten())\n",
        "\n",
        "# 4. Visualize the Final Result\n",
        "test_dates_start = look_back_window + train_size_lstm_split + val_size_lstm_split\n",
        "test_dates_slice = df_final['date'].iloc[test_dates_start : test_dates_start + len(true_test_lstm_tuned)]\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.plot(test_dates_slice, true_test_lstm_tuned.flatten(), label=\"Actual Price\", color='blue', linewidth=2)\n",
        "plt.plot(test_dates_slice, pred_test_lstm_tuned.flatten(), label=\"LSTM Tuned Prediction\", color='gold', linewidth=1.5, alpha=0.9)\n",
        "plt.title(\"FINAL EXAM: Tuned LSTM on Unseen Test Data (2024-2025)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price (USD)\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c0465d0"
      },
      "source": [
        "## Implement Time Series Cross-Validation\n",
        "\n",
        "\n",
        "Implement time series cross-validation using `TimeSeriesSplit` with the optimal `look_back_window` (60) to evaluate the LSTM model. Collect MAE, MSE, RMSE, and R^2 for each fold.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f9c7e2d",
        "outputId": "ea394f10-b73e-4c69-e04e-2199262b684d"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# 1. Set the optimal look_back_window to 60\n",
        "optimal_look_back_window = 60\n",
        "print(f\"Optimal look_back_window set to: {optimal_look_back_window}\")\n",
        "\n",
        "# 2. Initialize MinMaxScaler for features and target separately\n",
        "# Ensure df_final and feature_cols are the current ones with all added features\n",
        "scaler_tscv_features = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "#Fit minmax only on training portion of data\n",
        "x_train_scaled = scaler_tscv_features.fit_transform(train[feature_cols])\n",
        "#scaled_tscv_features = scaler_tscv_features.fit_transform(df_final[feature_cols])\n",
        "\n",
        "scaler_tscv_target = MinMaxScaler(feature_range=(0, 1))\n",
        "#scaled_tscv_target = scaler_tscv_target.fit_transform(df_final['target'].values.reshape(-1, 1))\n",
        "scaled_tscv_target = scaler_tscv_target.fit_transform(train['target'].values.reshape(-1, 1))\n",
        "\n",
        "#num_features_lstm = scaled_tscv_features.shape[1]\n",
        "num_features_lstm = x_train_scaled.shape[1]\n",
        "print(f\"Number of features for LSTM: {num_features_lstm}\")\n",
        "print(\"MinMaxScalers initialized and fitted for features and target.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "870c315a",
        "outputId": "9f8ef322-4a0d-4d0a-9401-3a70aea82f5d"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FIX: Remove ANY remaining NaN values before LSTM\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"PREPARING DATA FOR LSTM\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Rows before NaN removal: {len(df_final)}\")\n",
        "print(f\"NaN values per column:\")\n",
        "print(df_final[feature_cols + ['target']].isna().sum())\n",
        "\n",
        "# Remove ANY rows with NaN in features or target\n",
        "df_final = df_final.dropna(subset=feature_cols + ['target']).reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nRows after NaN removal: {len(df_final)}\")\n",
        "print(f\"Remaining NaN: {df_final[feature_cols + ['target']].isna().sum().sum()}\")\n",
        "\n",
        "# Verify no NaN remain\n",
        "assert df_final[feature_cols + ['target']].isna().sum().sum() == 0, \"Still have NaN values!\"\n",
        "\n",
        "print(\"✅ All NaN values removed\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# LSTM TIME SERIES CROSS-VALIDATION\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "optimal_look_back_window = 60\n",
        "n_splits = 2\n",
        "\n",
        "num_features_lstm = len(feature_cols)  # Should be 21\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "df = df_final.copy()\n",
        "\n",
        "# Reuse the create_lstm_sequences function\n",
        "def create_lstm_sequences(features_data, target_data, look_back):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(features_data) - look_back):\n",
        "        X_seq.append(features_data[i:(i + look_back), :])\n",
        "        y_seq.append(target_data[i + look_back, 0])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# Create an empty list to store the metrics for each fold\n",
        "fold_metrics = []\n",
        "\n",
        "# Loop through each split\n",
        "for fold, (train_index, val_index) in enumerate(tscv.split(df)):\n",
        "    print(f\"\\n===== FOLD {fold + 1} / {n_splits} ====\")\n",
        "\n",
        "    # Split raw data\n",
        "    train_fold = df.iloc[train_index]\n",
        "    val_fold = df.iloc[val_index]\n",
        "\n",
        "    # DEBUG: Check for NaN in raw data\n",
        "    print(f\"NaN in train_fold features: {train_fold[feature_cols].isna().sum().sum()}\")\n",
        "    print(f\"NaN in train_fold target: {train_fold['target'].isna().sum()}\")\n",
        "    print(f\"NaN in val_fold features: {val_fold[feature_cols].isna().sum().sum()}\")\n",
        "    print(f\"NaN in val_fold target: {val_fold['target'].isna().sum()}\")\n",
        "\n",
        "    # Initialize fresh scalers\n",
        "    scaler_X = MinMaxScaler()\n",
        "    scaler_y = MinMaxScaler()\n",
        "\n",
        "    # Fit and transform\n",
        "    X_train_scaled = scaler_X.fit_transform(train_fold[feature_cols])\n",
        "    y_train_scaled = scaler_y.fit_transform(train_fold[['target']])\n",
        "\n",
        "    X_val_scaled = scaler_X.transform(val_fold[feature_cols])\n",
        "    y_val_scaled = scaler_y.transform(val_fold[['target']])\n",
        "\n",
        "    # DEBUG: Check for NaN after scaling\n",
        "    print(f\"NaN after scaling X_train: {np.isnan(X_train_scaled).sum()}\")\n",
        "    print(f\"NaN after scaling y_train: {np.isnan(y_train_scaled).sum()}\")\n",
        "\n",
        "    # Create LSTM sequences\n",
        "    X_train_seq, y_train_seq = create_lstm_sequences(X_train_scaled, y_train_scaled, optimal_look_back_window)\n",
        "    X_val_seq, y_val_seq = create_lstm_sequences(X_val_scaled, y_val_scaled, optimal_look_back_window)\n",
        "\n",
        "    print(f\"Train sequences: {X_train_seq.shape}, Val sequences: {X_val_seq.shape}\")\n",
        "\n",
        "    if X_train_seq.shape[0] == 0 or X_val_seq.shape[0] == 0:\n",
        "        print(f\"Skipping fold {fold + 1} due to insufficient data.\")\n",
        "        continue\n",
        "\n",
        "    # Build LSTM model\n",
        "    model = Sequential([\n",
        "        Input(shape=(optimal_look_back_window, num_features_lstm)),\n",
        "        LSTM(64, return_sequences=True),\n",
        "        Dropout(0.2),\n",
        "        LSTM(64, return_sequences=False),\n",
        "        Dropout(0.2),\n",
        "        BatchNormalization(),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "\n",
        "    # Use LOWER learning rate\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0001, clipnorm=1.0), loss='huber')\n",
        "\n",
        "    # Callbacks\n",
        "    es = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True)\n",
        "    rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=9)\n",
        "\n",
        "    # Train\n",
        "    history = model.fit(\n",
        "        X_train_seq, y_train_seq,\n",
        "        epochs=20,\n",
        "        batch_size=32,\n",
        "        validation_data=(X_val_seq, y_val_seq),\n",
        "        callbacks=[es, rlr],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred_scaled = model.predict(X_val_seq, verbose=0)\n",
        "\n",
        "    # DEBUG: Check predictions\n",
        "    print(f\"NaN in predictions (scaled): {np.isnan(y_pred_scaled).sum()}\")\n",
        "    print(f\"Inf in predictions (scaled): {np.isinf(y_pred_scaled).sum()}\")\n",
        "\n",
        "    # Inverse transform\n",
        "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
        "    y_true = scaler_y.inverse_transform(y_val_seq.reshape(-1, 1))\n",
        "\n",
        "    # DEBUG: Check after inverse transform\n",
        "    print(f\"NaN in y_pred: {np.isnan(y_pred).sum()}\")\n",
        "    print(f\"NaN in y_true: {np.isnan(y_true).sum()}\")\n",
        "    print(f\"Inf in y_pred: {np.isinf(y_pred).sum()}\")\n",
        "\n",
        "    # If NaN or Inf exists, skip this fold\n",
        "    if np.isnan(y_pred).any() or np.isnan(y_true).any() or np.isinf(y_pred).any():\n",
        "        print(f\"⚠️ Skipping fold {fold + 1} due to NaN/Inf in predictions\")\n",
        "        continue\n",
        "\n",
        "    # Calculate metrics\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"Fold {fold + 1} Metrics - MAE: {mae:.2f}, MSE: {mse:.2f}, RMSE: {rmse:.2f}, R^2: {r2:.2f}\")\n",
        "\n",
        "    fold_metrics.append({\n",
        "        'fold': fold + 1,\n",
        "        'MAE': mae,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'R^2': r2\n",
        "    })\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Time Series Cross-Validation completed.\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if len(fold_metrics) > 0:\n",
        "    metrics_df_tscv = pd.DataFrame(fold_metrics)\n",
        "    print(\"\\n--- Time Series Cross-Validation Results ---\")\n",
        "    print(metrics_df_tscv)\n",
        "    print(\"\\n--- Average Metrics Across Folds ---\")\n",
        "    print(metrics_df_tscv.mean().to_frame('Average Metrics').transpose())\n",
        "else:\n",
        "    print(\"⚠️ No folds completed successfully\")\n",
        "\n",
        "print(\"\\n✅ LSTM Cross-Validation Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "CA6vukrB1QX_",
        "outputId": "4b4c4ebb-738c-4e4c-eda4-ac66c6abd403"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# LSTM TEST SET EVALUATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LSTM TEST SET EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Prepare test data the same way as in cross-validation\n",
        "scaler_X_test = MinMaxScaler()\n",
        "scaler_y_test = MinMaxScaler()\n",
        "\n",
        "# Fit scalers on train data only\n",
        "X_train_for_test = scaler_X_test.fit_transform(train[feature_cols])\n",
        "y_train_for_test = scaler_y_test.fit_transform(train[['target']])\n",
        "\n",
        "# Transform test data\n",
        "X_test_scaled = scaler_X_test.transform(test[feature_cols])\n",
        "y_test_scaled = scaler_y_test.transform(test[['target']])\n",
        "\n",
        "# Create LSTM sequences for test set\n",
        "X_test_seq, y_test_seq = create_lstm_sequences(\n",
        "    X_test_scaled,\n",
        "    y_test_scaled,\n",
        "    optimal_look_back_window\n",
        ")\n",
        "\n",
        "print(f\"Test sequences shape: {X_test_seq.shape}\")\n",
        "\n",
        "if X_test_seq.shape[0] > 0:\n",
        "    # Make predictions using the last trained model from CV\n",
        "    y_pred_test_scaled = model.predict(X_test_seq, verbose=0)\n",
        "\n",
        "    # Check for NaN\n",
        "    if np.isnan(y_pred_test_scaled).any():\n",
        "        print(\"⚠️ NaN found in test predictions!\")\n",
        "    else:\n",
        "        # Inverse transform to get actual prices\n",
        "        y_pred_test = scaler_y_test.inverse_transform(y_pred_test_scaled)\n",
        "        y_test_true = scaler_y_test.inverse_transform(y_test_seq.reshape(-1, 1))\n",
        "\n",
        "        # Evaluate\n",
        "        evaluate_model(\"LSTM - TEST SET\", y_test_true.flatten(), y_pred_test.flatten())\n",
        "\n",
        "        # Visualize test set predictions\n",
        "        # Get corresponding dates (skip first look_back_window rows)\n",
        "        test_dates_lstm = test['date'].iloc[optimal_look_back_window:optimal_look_back_window+len(y_test_true)].reset_index(drop=True)\n",
        "\n",
        "        plt.figure(figsize=(14,6))\n",
        "        plt.plot(test_dates_lstm, y_test_true.flatten(), label='Actual', linewidth=2, color='blue')\n",
        "        plt.plot(test_dates_lstm, y_pred_test.flatten(), label='LSTM Predicted', linewidth=1.5, alpha=0.8, color='orange')\n",
        "        plt.title('LSTM: Test Set Predictions (2024-2025)')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Price (USD)')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"⚠️ Not enough test data for LSTM sequences\")\n",
        "\n",
        "print(\"\\n✅ LSTM Test Evaluation Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3ab35362",
        "outputId": "aa61becc-ecf3-4a26-cc74-4f0698fdf821"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Descriptive Statistics for 'close' price ---\")\n",
        "for name, df_split in [('Train', train), ('Validation', val), ('Test', test)]:\n",
        "    print(f\"\\n{name} Set:\")\n",
        "    print(f\"  Mean 'close' price: ${df_split['close'].mean():.2f}\")\n",
        "    print(f\"  Standard Deviation 'close' price: ${df_split['close'].std():.2f}\")\n",
        "    print(f\"  Min 'close' price: ${df_split['close'].min():.2f}\")\n",
        "    print(f\"  Max 'close' price: ${df_split['close'].max():.2f}\")\n",
        "\n",
        "print(\"\\n--- Temporal Ranges ---\")\n",
        "print(f\"Train dates: {train['date'].min()} to {train['date'].max()}\")\n",
        "print(f\"Val dates: {val['date'].min()} to {val['date'].max()}\")\n",
        "print(f\"Test dates: {test['date'].min()} to {test['date'].max()}\")\n",
        "\n",
        "# Features for histogram analysis\n",
        "selected_features = ['close_lag_1', 'Volume BTC', 'RSI']\n",
        "\n",
        "print(\"\\n--- Histograms for Selected Features Across Splits ---\")\n",
        "for feature in selected_features:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(train[feature], color='blue', label='Train', kde=True, stat='density', alpha=0.5)\n",
        "    sns.histplot(val[feature], color='green', label='Validation', kde=True, stat='density', alpha=0.5)\n",
        "    sns.histplot(test[feature], color='red', label='Test', kde=True, stat='density', alpha=0.5)\n",
        "    plt.title(f'Distribution of {feature}')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "1b85ba23",
        "outputId": "d2366f11-53aa-415f-c741-28064fefb17c"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,6))\n",
        "plt.plot(train['date'], train['close'], label='Train', alpha=0.7, color='blue')\n",
        "plt.plot(val['date'], val['close'], label='Validation', alpha=0.7, color='green')\n",
        "plt.plot(test['date'], test['close'], label='Test', alpha=0.7, linewidth=2, color='red')\n",
        "plt.title('BTC Close Price Over Time: Train, Validation, and Test Splits')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('BTC Price')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW7X29rPdF6N"
      },
      "outputs": [],
      "source": [
        "# Create log returns\n",
        "df_final['log_price'] = np.log(df_final['close'])\n",
        "df_final['target'] = df_final['log_price'].shift(-7) - df_final['log_price']\n",
        "\n",
        "# drop final row\n",
        "df_final = df_final.dropna().reset_index(drop=True)\n",
        "\n",
        "# feature_cols = [...]  # your chosen features\n",
        "\n",
        "X_all = df_final[feature_cols].values\n",
        "y_all = df_final['target'].values.reshape(-1, 1)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "n_splits = 2   # or 5\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "# Building LSTM sequences\n",
        "def create_sequences(features, target, look_back):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(features) - look_back):\n",
        "        X_seq.append(features[i:i+look_back])\n",
        "        y_seq.append(target[i+look_back])\n",
        "    return np.array(X_seq), np.array(y_seq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XojAaTaYdYp8",
        "outputId": "e0f70887-b9c4-452f-f54f-2f92f326c2cd"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import Huber\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from tensorflow.keras.layers import Input # Import Input layer for explicit shape definition\n",
        "from tensorflow.keras.layers import BatchNormalization # Import BatchNormalization\n",
        "\n",
        "look_back = 60\n",
        "fold_results = []\n",
        "fold_num = 1\n",
        "\n",
        "for train_index, val_index in tscv.split(X_all):\n",
        "    print(f\"\\n-------------------\")\n",
        "    print(f\"Processing Fold {fold_num}/{n_splits}\")\n",
        "    print(f\"Train: {len(train_index)}, Val: {len(val_index)}\")\n",
        "    print(f\"-------------------\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # 5A: Split data for this fold\n",
        "    # -----------------------------\n",
        "    X_train_raw = X_all[train_index]\n",
        "    y_train_raw = y_all[train_index]\n",
        "\n",
        "    X_val_raw = X_all[val_index]\n",
        "    y_val_raw = y_all[val_index]\n",
        "\n",
        "    # -----------------------------\n",
        "    # 5B: Scale using ONLY training data\n",
        "    # -----------------------------\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train_raw)\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train_raw)\n",
        "\n",
        "    X_val_scaled = scaler_X.transform(X_val_raw)\n",
        "    y_val_scaled = scaler_y.transform(y_val_raw)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 5C: Create sequences\n",
        "    # -----------------------------\n",
        "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, look_back)\n",
        "    X_val_seq, y_val_seq     = create_sequences(X_val_scaled, y_val_scaled, look_back)\n",
        "\n",
        "    # Handle cases where sequence creation might result in empty arrays\n",
        "    if X_train_seq.shape[0] == 0 or X_val_seq.shape[0] == 0:\n",
        "        print(f\"Skipping fold {fold_num} due to insufficient data for sequence creation after look_back window.\")\n",
        "        fold_num += 1\n",
        "        continue\n",
        "\n",
        "    # -----------------------------\n",
        "    # 5D: Build model (stable setup)\n",
        "    # -----------------------------\n",
        "    model = Sequential([\n",
        "        Input(shape=(look_back, X_train_seq.shape[2])),\n",
        "        LSTM(64, return_sequences=True), # First LSTM returns sequences\n",
        "        BatchNormalization(), # Add BatchNormalization\n",
        "        #Dropout(0.2),\n",
        "        LSTM(64, return_sequences=False), # Last LSTM does not return sequences\n",
        "        BatchNormalization(), # Add BatchNormalization\n",
        "        #Dropout(0.2),\n",
        "        Dense(32, activation='relu'), # Using relu activation\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.0005, clipnorm=1.0),\n",
        "        loss=Huber(delta=1.0)\n",
        "    )\n",
        "\n",
        "    es = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True)\n",
        "    rlr = ReduceLROnPlateau(monitor='val_loss', patience=6, factor=0.5)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 5E: Train\n",
        "    # -----------------------------\n",
        "    model.fit(\n",
        "        X_train_seq, y_train_seq,\n",
        "        validation_data=(X_val_seq, y_val_seq),\n",
        "        epochs=95,\n",
        "        batch_size=70,\n",
        "        callbacks=[es, rlr],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # -----------------------------\n",
        "    # 5F: Predict + inverse transform\n",
        "    # -----------------------------\n",
        "    y_pred_scaled = model.predict(X_val_seq, verbose=0)\n",
        "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
        "    y_true = scaler_y.inverse_transform(y_val_seq)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 5G: Compute metrics\n",
        "    # -----------------------------\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "    fold_results.append((fold_num, r2, mse, rmse, mae))\n",
        "\n",
        "    print(f\"Fold {fold_num} — R²: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
        "\n",
        "    fold_num += 1\n",
        "\n",
        "\n",
        "\n",
        "# Results summary\n",
        "print(\"\\n====== FINAL RESULTS ======\")\n",
        "for fold, r2, mse, rmse, mae in fold_results:\n",
        "    print(f\"Fold {fold}: R²={r2:.4f}, RMSE={rmse:.4f}, MAE={mae:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
